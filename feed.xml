<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>LAB</title>
    <link href="https://www.k8s.it/feed.xml" rel="self" />
    <link href="https://www.k8s.it" />
    <updated>2020-08-11T20:47:10+02:00</updated>
    <author>
        <name>lgirardi</name>
    </author>
    <id>https://www.k8s.it</id>

    <entry>
        <title>Kubernetes-guacamole</title>
        <author>
            <name>lgirardi</name>
        </author>
        <link href="https://www.k8s.it/kubernetes-guacamole.html"/>
        <id>https://www.k8s.it/kubernetes-guacamole.html</id>

        <updated>2020-08-11T20:46:54+02:00</updated>
            <summary>
                <![CDATA[
                    Here we are , another apache guacamole implementation in kubernetesThis service is designed to avoid the usage of mysql and create a standalone project The main idea is to use the user-mapping.xml as a config map For production environment i suggest to add the ldap auth (ad.openldap,freeipa),&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <div class="flex-shrink-0 col-12 col-md-9 mb-4 mb-md-0">
<div id="readme" class="Box md js-code-block-container Box--responsive">
<div class="Box-body px-5 pb-5">
<article class="markdown-body entry-content container-lg">
<h2 id="mcetoc_1effdgcnt5">Here we are , another apache guacamole implementation in kubernetes</h2>
<p>This service is designed to avoid the usage of mysql and create a standalone project</p>
<p>The main idea is to use the <strong>user-mapping.xml</strong> as a config map</p>
<p>For production environment i suggest to add the ldap auth (ad.openldap,freeipa),<br>mysql database should be managed with a dedicated instances and mantained in case of "exit"</p>
<h2 id="mcetoc_1effdg4pk0"><a id="user-content-what-is-a-bastion-host" class="anchor" aria-hidden="true" href="https://github.com/lorenzogirardi/kubernetes-guacamole#what-is-a-bastion-host"><svg class="octicon octicon-link" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>what is a bastion host</h2>
<p>On the Internet, a bastion host is the only host computer that a company allows to be addressed<br>directly from the public network and that is designed to screen the rest of its network from security exposure.</p>
<h2 id="mcetoc_1effdg4pk1"><a id="user-content-how-this-tool-can-be-used" class="anchor" aria-hidden="true" href="https://github.com/lorenzogirardi/kubernetes-guacamole#how-this-tool-can-be-used"><svg class="octicon octicon-link" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>how this tool can be used</h2>
<p>The tool is designed to be used when you have some dedicated service in production and you have to keep<br>the control of access and account used , guacamole has the ability to manage the most used platforms (windows and linux)<br>as host in backend to be reached from developers ... contractors ...</p>
<h2 id="mcetoc_1effdg4pk2"><a id="user-content-why-in-kubernetes" class="anchor" aria-hidden="true" href="https://github.com/lorenzogirardi/kubernetes-guacamole#why-in-kubernetes"><svg class="octicon octicon-link" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>why in kubernetes</h2>
<p>Since the auth method could scale by configmap or ldap or mysql , is designed to scale<br>we have also the benefits to have a low footprint compared to a traditional vm.</p>
<h2 id="mcetoc_1effdg4pk3"><a id="user-content-config-to-change" class="anchor" aria-hidden="true" href="https://github.com/lorenzogirardi/kubernetes-guacamole#config-to-change"><svg class="octicon octicon-link" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>config to change</h2>
<p>Before deploy you need to specify the following parameters in guacamole folder</p>
<ul>
<li>YOUR_DOMAIN to reflect your domain url in 03-guacamole-ing.yaml</li>
<li>user YOUR_USERNAME / YOUR_MD5_PWD and hosts xml configuration in 04-guacamole-cfm.yaml following <a href="https://guacamole.apache.org/doc/gug/configuring-guacamole.html#user-mapping" rel="nofollow">https://guacamole.apache.org/doc/gug/configuring-guacamole.html#user-mapping</a></li>
</ul>
<p>screenshots</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/905e74d0ce81c8b239f67d405567100a3f181292/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f76313538303835303535322f6d6973632f67756163616d6f6c652d77696e2e706e67"><img src="https://camo.githubusercontent.com/905e74d0ce81c8b239f67d405567100a3f181292/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f76313538303835303535322f6d6973632f67756163616d6f6c652d77696e2e706e67" alt="windows" ></a><br><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/fd53b1991cd9693679aae9aadf72bc75d025c2ad/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f76313538303835303535322f6d6973632f67756163616d6f6c652d6c696e75782e706e67"><img src="https://camo.githubusercontent.com/fd53b1991cd9693679aae9aadf72bc75d025c2ad/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f76313538303835303535322f6d6973632f67756163616d6f6c652d6c696e75782e706e67" alt="linux" ></a></p>
<h2 id="mcetoc_1effdg4pk4"><a id="user-content-deploy" class="anchor" aria-hidden="true" href="https://github.com/lorenzogirardi/kubernetes-guacamole#deploy"><svg class="octicon octicon-link" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>deploy</h2>
<p id="mcetoc_1effdgp3l6"><code>kubectl apply -f guacd</code></p>
<p id="mcetoc_1effdgsfc7"><code>kubectl apply -f guacamole</code></p>
<p>You can secure the connection with kube-lego and use cillium to add network rules </p>
</article>
</div>
</div>
</div>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Kubernetes-strongswan</title>
        <author>
            <name>lgirardi</name>
        </author>
        <link href="https://www.k8s.it/kubernetes-strongswan.html"/>
        <id>https://www.k8s.it/kubernetes-strongswan.html</id>

        <updated>2020-08-11T20:47:06+02:00</updated>
            <summary>
                <![CDATA[
                    How we can manage vpn in kubernetes environmentHi there , this project is to cover the vpn ipsec-xauth topic in a kubernetes evironment, the goal of this is to have the less effort possible when we have to manage users. Architecture Requirements: WHYThe traditional ipsec-xauth&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <div class="flex-shrink-0 col-12 col-md-9 mb-4 mb-md-0">
<div id="readme" class="Box md js-code-block-container Box--responsive">
<div class="Box-body px-5 pb-5">
<article class="markdown-body entry-content container-lg">
<h2 id="mcetoc_1effdcouu0">How we can manage vpn in kubernetes environment</h2>
<p>Hi there , this project is to cover the vpn ipsec-xauth topic in a kubernetes evironment,<br>the goal of this is to have the less effort possible when we have to manage users.</p>
<p>Architecture<br><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ee7bbe0fe8e2f17b0bda45b6b92d585105d9adcc/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f76313538343733313733352f6d6973632f76706e5f6469616772616d2e6a7067"><img src="https://camo.githubusercontent.com/ee7bbe0fe8e2f17b0bda45b6b92d585105d9adcc/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f76313538343733313733352f6d6973632f76706e5f6469616772616d2e6a7067" alt="architecture" ></a></p>
<p>Requirements:</p>
<ul>
<li>Kubernetes</li>
<li>Strongswan</li>
<li>Microsoft Acrive Directory / openldap / freeipa etc etc LDAP (i'll use ldap instead the software name)</li>
</ul>
<h2 id="mcetoc_1effdcouu1"><a id="user-content-why" class="anchor" aria-hidden="true" href="https://github.com/lorenzogirardi/kubernetes-strongswan#why"><svg class="octicon octicon-link" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>WHY</h2>
<p>The traditional ipsec-xauth vpn with ikev1 is based on PSK<br>and a client username/password , this is a problem when the credential are stored in a file<br>in kubernetes update a file always mean rollout a new deploy or create a procedure to<br>make effective the changes.<br>So the idea is to deploy something that doesn't need any interaction<br>after the deploy and manage the clients, with the company standards,<br>like password expiration, password complexity, groups attributions and so on.</p>
<h2 id="mcetoc_1effdcouu2"><a id="user-content-how" class="anchor" aria-hidden="true" href="https://github.com/lorenzogirardi/kubernetes-strongswan#how"><svg class="octicon octicon-link" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>HOW</h2>
<p>In order to have a fully managed services we can leverage the usage of ldap procedures (that all company has).<br>Strongswan (a fork of *swan ipsec software) could be integrated with ldap with pam.<br>pam is ... well --&gt; <a href="https://tldp.org/HOWTO/User-Authentication-HOWTO/x115.html" rel="nofollow">https://tldp.org/HOWTO/User-Authentication-HOWTO/x115.html</a></p>
<p>So what we need in ldap ?<br>We need:</p>
<ul>
<li>a technical user that is a low level profile that will be used only to check the users inside the ldap tree and the groups associated</li>
<li>a group to associate to people who need/granted the vpn access</li>
</ul>
<p>in this scenario tech users is --&gt; <em>ldapbind</em><br>group is --&gt; <em>vpn</em></p>
<p>here some screen related</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/48427e34136d63b4b7348ba8c2148e178665cdf0/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f635f7363616c652c775f3634302f76313538343733303832382f6d6973632f7374726f6e677377616e5f62696e645f757365722e706e67"><img src="https://camo.githubusercontent.com/48427e34136d63b4b7348ba8c2148e178665cdf0/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f635f7363616c652c775f3634302f76313538343733303832382f6d6973632f7374726f6e677377616e5f62696e645f757365722e706e67" alt="ldapbind" ></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/141e29aaed92e477719b517dd704c815b18c23bc/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f635f7363616c652c775f3634302f76313538343733303832382f6d6973632f7374726f6e677377616e5f757365725f76706e5f67726f75702e706e67"><img src="https://camo.githubusercontent.com/141e29aaed92e477719b517dd704c815b18c23bc/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f635f7363616c652c775f3634302f76313538343733303832382f6d6973632f7374726f6e677377616e5f757365725f76706e5f67726f75702e706e67" alt="vpn group" ></a></p>
<p>Then... now we have to configure configure the Docker image in order to support pam ldap.</p>
<p>Dockerfile</p>
<pre><code>FROM debian:stretch
MAINTAINER lgirardi &lt;l@k8s.it&gt;


RUN apt-get -y update &amp;&amp; apt-get -yq install \
        strongswan \
        libcharon-extra-plugins \
        iptables \
        kmod \
        libpam-ldap \
        vim

EXPOSE 500/udp 4500/udp

CMD /usr/sbin/ipsec start --nofork
</code></pre>
<p>libpam-ldap and libcharon-extra-plugins are what we need to perform this kind of integration.</p>
<p>Since strongswan is not traditionally used in kubernetes , has some files that needs a configuration.<br>ENV variables are the most useful to configure it,<br>unfortunately the process is not able to share the env this the child process,<br>so we will work with 2 concepts,<br>use the configmap for all files we need to configure use the secrets for all sensitive data we need to add</p>
<p>files configured:</p>
<ul>
<li>ipsec.conf (the strongswan main configuration)</li>
<li>xauth-pam.conf (strongswan configuration to enable pam)</li>
<li>attr.conf (strongswan configuration file for split-tunnel)<br><em>split-tunnel is when you want to move in vpn only the company subnet and use the home gateway for all the other usages</em></li>
<li>ipsec (pam configuration in /etc/pam.d)</li>
</ul>
<p>secrets:</p>
<ul>
<li>ipsec.secrets (file with the ipsec PSK) rif. 003-configmap.yaml</li>
<li>pam_ldap.conf (configuration used by pam module to connect to ldap) rif. 002-secrets.yaml</li>
</ul>
<p><em>remember that all secrets files are managed using base64 encoding</em></p>
<p>When we have multiple files to spread in different locations we have to create some tricks,<br>one is to create symlink in the Dockerfile , however we have to keep the configuration<br>as much as possible agnostic from the Dockerfile.</p>
<p><em>volume</em> and <em>volumeMounts</em> can help on this topic</p>
<pre><code>volumeMounts:
- name: psk
  mountPath: /etc/ipsec.secrets
  subPath: psk
  readOnly: true
- name: pamldap
  mountPath: /etc/pam_ldap.conf
  subPath: pamldap
- name: strongswan-attr
  mountPath: /etc/strongswan.d/charon/attr.conf
  subPath: attr.conf
- name: strongswan-xauth-pam
  mountPath: /etc/strongswan.d/charon/xauth-pam.conf
  subPath: xauth-pam.conf
- name: strongswan-ipsec
  mountPath: /etc/pam.d/ipsec
  subPath: ipsec
- name: strongswan-ipseconf
  mountPath: /etc/ipsec.conf
  subPath: ipsec.conf
volumes:
- name: strongswan-attr
  configMap:
    name: strongswanconfigmap
    items:
    - key: attr.conf
      path: attr.conf
- name: strongswan-xauth-pam
  configMap:
    name: strongswanconfigmap
    items:
    - key: xauth-pam.conf
      path: xauth-pam.conf
- name: strongswan-ipsec
  configMap:
    name: strongswanconfigmap
    items:
    - key: ipsec
      path: ipsec
- name: strongswan-ipseconf
  configMap:
    name: strongswanconfigmap
    items:
    - key: ipsec.conf
      path: ipsec.conf
- name: psk
  secret:
    secretName: strongswan-secret
- name: pamldap
  secret:
    secretName: strongswan-secret
</code></pre>
<p>Now we have all configured, we can just run<br><code>kubectl apply -f deploy</code></p>
<p>We will have soon a pod into strongswan namespace</p>
<pre><code># kubectl get pods -n strongswan
NAME                          READY   STATUS    RESTARTS   AGE
strongswan-77bfbb9f9f-57hmz   1/1     Running   0          22h
</code></pre>
<p>Since the service is configured with nodport we need to enable the default 500 and 4500<br>in our firewall , matching the kubernetes ports 30000-32767, in this this service are</p>
<pre><code>ports:
- name: isakmp-udp
  protocol: UDP
  nodePort: 30500
  port: 500
  targetPort: 500
- name: ipsec-nat-t
  protocol: UDP
  nodePort: 30450
  port: 4500
  targetPort: 4500
type: NodePort
</code></pre>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/9a42ba23f208fb8562105b7b19d240bc5e495426/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f635f7363616c652c775f3634302f76313538343733303832372f6d6973632f7374726f6e677377616e5f6669726577616c6c5f6e61742e706e67"><img src="https://camo.githubusercontent.com/9a42ba23f208fb8562105b7b19d240bc5e495426/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f635f7363616c652c775f3634302f76313538343733303832372f6d6973632f7374726f6e677377616e5f6669726577616c6c5f6e61742e706e67" alt="firewall configuration" ></a></p>
<p>We can configure our standard client (cisco ipsec client is enough)</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ace3ff6a96cf31968f11209b7b05fce4ea5487a8/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f635f7363616c652c775f3332302f76313538343733353439332f6d6973632f7374726f6e677377616e5f616e64726f69645f636c69656e742e706e67"><img src="https://camo.githubusercontent.com/ace3ff6a96cf31968f11209b7b05fce4ea5487a8/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f635f7363616c652c775f3332302f76313538343733353439332f6d6973632f7374726f6e677377616e5f616e64726f69645f636c69656e742e706e67" alt="android configuration" ></a></p>
<pre><code># kubectl logs strongswan-77bfbb9f9f-57hmz -n strongswan
Starting strongSwan 5.5.1 IPsec [starter]...
no netkey IPsec stack detected
no KLIPS IPsec stack detected
no known IPsec stack detected, ignoring!
charon (13) started after 80 ms
00[DMN] Starting IKE charon daemon (strongSwan 5.5.1, Linux 4.15.0-70-generic, x86_64)
00[CFG] mapping attribute type split-exclpude failed
00[CFG] loading ca certificates from '/etc/ipsec.d/cacerts'
00[CFG] loading aa certificates from '/etc/ipsec.d/aacerts'
00[CFG] loading ocsp signer certificates from '/etc/ipsec.d/ocspcerts'
00[CFG] loading attribute certificates from '/etc/ipsec.d/acerts'
00[CFG] loading crls from '/etc/ipsec.d/crls'
00[CFG] loading secrets from '/etc/ipsec.secrets'
00[CFG]   loaded IKE secret for %any
00[CFG] loaded 0 RADIUS server configurations
00[CFG] HA config misses local/remote address
00[LIB] loaded plugins: charon aes rc2 sha2 sha1 md5 random nonce x509 revocation constraints pubkey pkcs1 pkcs7 pkcs8 pkcs12 pgp dnskey sshkey pem openssl fips-prf gmp agent xcbc hmac gcm attr kernel-netlink resolve socket-default connmark farp stroke updown eap-identity eap-aka eap-md5 eap-gtc eap-mschapv2 eap-radius eap-tls eap-ttls eap-tnc xauth-generic xauth-eap xauth-pam tnc-tnccs dhcp lookip error-notify certexpire led addrblock unity
00[LIB] dropped capabilities, running as uid 0, gid 0
00[JOB] spawning 16 worker threads
05[CFG] received stroke: add connection 'roadw'
05[CFG] adding virtual IP address pool 172.16.17.0/29
05[CFG] added configuration 'roadw'
08[NET] received packet: from 10.1.1.1[36312] to 10.1.1.84[500] (756 bytes)
08[ENC] parsed ID_PROT request 0 [ SA V V V V V V V V ]
08[IKE] received NAT-T (RFC 3947) vendor ID
08[IKE] received draft-ietf-ipsec-nat-t-ike-02 vendor ID
08[IKE] received draft-ietf-ipsec-nat-t-ike-02\n vendor ID
08[IKE] received draft-ietf-ipsec-nat-t-ike-00 vendor ID
08[IKE] received XAuth vendor ID
08[IKE] received Cisco Unity vendor ID
08[IKE] received FRAGMENTATION vendor ID
08[IKE] received DPD vendor ID
08[IKE] 10.1.1.1 is initiating a Main Mode IKE_SA
08[ENC] generating ID_PROT response 0 [ SA V V V V ]
08[NET] sending packet: from 10.1.1.84[500] to 10.1.1.1[36312] (160 bytes)
05[NET] received packet: from 10.1.1.1[36312] to 10.1.1.84[500] (228 bytes)
05[ENC] parsed ID_PROT request 0 [ KE No NAT-D NAT-D ]
05[IKE] local host is behind NAT, sending keep alives
05[IKE] remote host is behind NAT
05[ENC] generating ID_PROT response 0 [ KE No NAT-D NAT-D ]
05[NET] sending packet: from 10.1.1.84[500] to 10.1.1.1[36312] (244 bytes)
09[NET] received packet: from 10.1.1.1[40011] to 10.1.1.84[4500] (92 bytes)
09[ENC] parsed ID_PROT request 0 [ ID HASH ]
09[CFG] looking for XAuthInitPSK peer configs matching 10.1.1.84...10.1.1.1[100.106.113.62]
09[CFG] selected peer config "roadw"
09[ENC] generating ID_PROT response 0 [ ID HASH ]
09[NET] sending packet: from 10.1.1.84[4500] to 10.1.1.1[40011] (92 bytes)
09[ENC] generating TRANSACTION request 3276308191 [ HASH CPRQ(X_USER X_PWD) ]
09[NET] sending packet: from 10.1.1.84[4500] to 10.1.1.1[40011] (76 bytes)
11[NET] received packet: from 10.1.1.1[40011] to 10.1.1.84[4500] (108 bytes)
11[ENC] parsed TRANSACTION response 3276308191 [ HASH CPRP(X_USER X_PWD) ]
11[IKE] PAM authentication of 'lgirardi' successful
11[IKE] XAuth authentication of 'lgirardi' successful
11[ENC] generating TRANSACTION request 1380277626 [ HASH CPS(X_STATUS) ]
11[NET] sending packet: from 10.1.1.84[4500] to 10.1.1.1[40011] (76 bytes)
10[NET] received packet: from 10.1.1.1[40011] to 10.1.1.84[4500] (108 bytes)
10[ENC] parsed INFORMATIONAL_V1 request 4006980307 [ HASH N(INITIAL_CONTACT) ]
12[NET] received packet: from 10.1.1.1[40011] to 10.1.1.84[4500] (92 bytes)
12[ENC] parsed TRANSACTION response 1380277626 [ HASH CPA(X_STATUS) ]
12[IKE] IKE_SA roadw[1] established between 10.1.1.84[ETHZERO_HOME_VPN]...10.1.1.1[100.106.113.62]
12[IKE] scheduling rekeying in 86047s
12[IKE] maximum IKE_SA lifetime 86227s
</code></pre>
<p>ok now i'm connected and i can see my network in tun0</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/0ec58f59178a15278c87059d926b2a8b7b2938a8/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f635f7363616c652c775f3332302f76313538343733303936302f6d6973632f7374726f6e677377616e5f636c69656e745f616e64726f69642e6a7067"><img src="https://camo.githubusercontent.com/0ec58f59178a15278c87059d926b2a8b7b2938a8/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f635f7363616c652c775f3332302f76313538343733303936302f6d6973632f7374726f6e677377616e5f636c69656e745f616e64726f69642e6a7067" alt="android ip" ></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/89492159c8fb5380197b4ecd266dce5c4068a9a0/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f635f7363616c652c775f3332302f76313538343733363033342f6d6973632f7374726f6e677377616e5f616e64726f69645f70696e672e6a7067"><img src="https://camo.githubusercontent.com/89492159c8fb5380197b4ecd266dce5c4068a9a0/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f635f7363616c652c775f3332302f76313538343733363033342f6d6973632f7374726f6e677377616e5f616e64726f69645f70696e672e6a7067" alt="android ping" ></a></p>
<p>Thats all ... here my connection</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/b702d2df92e56ffae8f754aed08e84ebd4d0595f/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f635f7363616c652c775f3634302f76313538343733363332382f6d6973632f7374726f6e677377616e5f7374726f6b655f737461747573616c6c2e706e67"><img src="https://camo.githubusercontent.com/b702d2df92e56ffae8f754aed08e84ebd4d0595f/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f635f7363616c652c775f3634302f76313538343733363332382f6d6973632f7374726f6e677377616e5f7374726f6b655f737461747573616c6c2e706e67" alt="strongswan connection" ></a></p>
</article>
</div>
</div>
</div>
            ]]>
        </content>
    </entry>
    <entry>
        <title>docker-latency</title>
        <author>
            <name>lgirardi</name>
        </author>
        <link href="https://www.k8s.it/docker-latency.html"/>
        <id>https://www.k8s.it/docker-latency.html</id>

        <updated>2020-08-11T20:42:02+02:00</updated>
            <summary>
                <![CDATA[
                    aka the network blaming toolSo again another grafana stack with docker Well yes but with a precise scope In this period we are almost all working from home, the blaming topic is usually the connection with our offices or the datacenters. Is not so rare&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2 id="mcetoc_1effd75jsc"><a id="user-content-aka-the-network-blaming-tool" class="anchor" aria-hidden="true" href="https://github.com/lorenzogirardi/docker-latency#aka-the-network-blaming-tool"><svg class="octicon octicon-link" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>aka the network blaming tool</h2>
<p>So again another grafana stack with docker<br>Well yes but with a precise scope</p>
<p>In this period we are almost all working from home,<br>the blaming topic is usually the connection with our offices or the datacenters.</p>
<p>Is not so rare for a network Administrator hear people that sais ,<br><em>the vpn is slow</em> , <em>i cannot connect to ... $something</em> , bla bla bla</p>
<p>In my experience this is usually due to the quality of the provider,<br>sometimes is also a problem on route path on T2/T3 providers</p>
<h3 id="mcetoc_1effd75jte"><a id="user-content-how-we-can-undestand-if-our-network-is-really-slow-" class="anchor" aria-hidden="true" href="https://github.com/lorenzogirardi/docker-latency#how-we-can-undestand-if-our-network-is-really-slow-"><svg class="octicon octicon-link" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>HOW we can undestand if our network is really slow ?</h3>
<p>The idea is to start a grafana stack ready-made to handle the basics statistics of our internet connection.<br>We need to choose some endpoints to monitor, example , your vpn endpoint , your datacenter/office public ip , the main dns servers and so on</p>
<h4 id="mcetoc_1effd75jtf"><a id="user-content-requirements" class="anchor" aria-hidden="true" href="https://github.com/lorenzogirardi/docker-latency#requirements"><svg class="octicon octicon-link" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Requirements</h4>
<ul>
<li>Docker</li>
<li>Docker Compose</li>
</ul>
<h4 id="mcetoc_1effd75jtg"><a id="user-content-stack" class="anchor" aria-hidden="true" href="https://github.com/lorenzogirardi/docker-latency#stack"><svg class="octicon octicon-link" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Stack</h4>
<ul>
<li>Influxdb</li>
<li>Grafana</li>
<li>Telegraf</li>
</ul>
<h4 id="mcetoc_1effd75jth"><a id="user-content-tree" class="anchor" aria-hidden="true" href="https://github.com/lorenzogirardi/docker-latency#tree"><svg class="octicon octicon-link" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Tree</h4>
<pre><code>├── .env
├── Makefile
├── README.md
├── docker
│   ├── grafana
│   │   ├── Dashboard-PING.json
│   │   ├── dashboard.yaml
│   │   └── datasource.yaml
│   ├── influxdb
│   │   ├── influxdb.conf
│   │   
│   └── telegraf
│       └── telegraf.conf
├── docker-compose.yml
</code></pre>
<p>Makefile is ... well a makefile , commands allowed<br><em>up , down, dev, down, logs, clean</em><br>up is to startup the stack<br>down to shutdown clean is done to remove also the storage saved for influxdb and grafana</p>
<p>.env contains the grafana and influxdb credentials (yes the default password is quite complicated)<br>Since this tool is hosted in your laptop (could be everywhere), never mind the <em>security</em></p>
<pre><code>GRAFANA_USER=admin
GRAFANA_PASSWORD=EQyFJpjxvJG8k2K8
INFLUXDB_DOMAIN=influxdb
INFLUXDB_DATABASE=ping
</code></pre>
<h3 id="mcetoc_1effd75jti"><a id="user-content-configuration" class="anchor" aria-hidden="true" href="https://github.com/lorenzogirardi/docker-latency#configuration"><svg class="octicon octicon-link" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Configuration</h3>
<p>We just need to choose the endpoints we'd like to monitor from our internet connection This could be done editing <em>telegraf.conf</em></p>
<pre><code>[global_tags]
[agent]
  interval = "10s"
  round_interval = true
  metric_batch_size = 1000
  metric_buffer_limit = 10000
  collection_jitter = "0s"
  flush_interval = "10s"
  flush_jitter = "0s"
  precision = ""
  hostname = "local-telegraf"
  omit_hostname = false
[[outputs.influxdb]]
   urls = ["http://127.0.0.1:8086"]
   database = "ping"
[[inputs.ping]]
urls = ["1.1.1.1", "8.8.8.8", "208.67.222.222", "test1.velocable.com"]
count = 7
ping_interval = 1.0
</code></pre>
<p>Edit <em>urls =</em> adding / modify the endpoints<br>(in this example, Cloudflare dns , Google dns, opendns, and a server in Madrid used for speedtest)</p>
<p>The configuration is collecting information every 10 seconds , and run a ping command 7 time each with 1 second delay.</p>
<h3 id="mcetoc_1effd75jtj"><a id="user-content-startup" class="anchor" aria-hidden="true" href="https://github.com/lorenzogirardi/docker-latency#startup"><svg class="octicon octicon-link" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Startup</h3>
<p>Inside the main folder run</p>
<p><code>make up</code></p>
<p>output:</p>
<pre><code>docker-latency$ make up
docker-compose -f docker-compose.yml up -d
Creating network "docker-latency_default" with the default driver
Creating grafana  ... done
Creating influxdb ... done
Creating telegraf ... done
</code></pre>
<p>login to:<br><code>http://localhost:3000/ </code>admin/EQyFJpjxvJG8k2K8</p>
<p>you will see</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/56bea14df7a89b07f4e319b45ad4d27c39ad865f/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f76313538353634313235322f6d6973632f67726166616e615f686f6d652e706e67"><img src="https://camo.githubusercontent.com/56bea14df7a89b07f4e319b45ad4d27c39ad865f/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f76313538353634313235322f6d6973632f67726166616e615f686f6d652e706e67" alt="grafana_home" ></a></p>
<p>than , checking for the only board present --&gt; <em>internet latency</em></p>
<p>you will have all details about the endpoint chosen , packet loss especially</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/5fd87b371e34b4497018003fcd41cd2e09d23c72/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f76313538353539353832342f6d6973632f67726166616e615f70696e672e706e67"><img src="https://camo.githubusercontent.com/5fd87b371e34b4497018003fcd41cd2e09d23c72/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f6574687a65726f2f696d6167652f75706c6f61642f76313538353539353832342f6d6973632f67726166616e615f70696e672e706e67" alt="grafana_ping" ></a></p>
<p>100% packet loss simulated disabling network card for few seconds.<br>The dashboard is using variables in order to create 1 row for each endpoint.</p>
<h3 id="mcetoc_1effd75jtk"><a id="user-content-conclusion" class="anchor" aria-hidden="true" href="https://github.com/lorenzogirardi/docker-latency#conclusion"><svg class="octicon octicon-link" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Conclusion</h3>
<p>Now we have data, so we know what is going on in our internet connection and we can probably<br>have more details about the <em>infomagic</em> words like ... <em>is slow</em></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Terraform your free Claudflare account</title>
        <author>
            <name>lgirardi</name>
        </author>
        <link href="https://www.k8s.it/terraform-your-free-claudflare-account.html"/>
        <id>https://www.k8s.it/terraform-your-free-claudflare-account.html</id>
            <category term="waf"/>
            <category term="terraform"/>
            <category term="gitlab"/>
            <category term="cloudflare"/>
            <category term="cdn"/>
            <category term="automation"/>

        <updated>2018-07-07T11:47:37+02:00</updated>
            <summary>
                <![CDATA[
                    So... after some experience with akamai www.ethzero.it is an alias for aku-cs-akamaiflowers.com.edgesuite.net. aku-cs-akamaiflowers.com.edgesuite.net is an alias for a169.dscksd.akamai.net. a169.dscksd.akamai.net has address 193.45.15.98 a169.dscksd.akamai.net has address 193.45.15.122 a169.dscksd.akamai.net has IPv6 address 2001:2030:0:27::c12d:f62 a169.dscksd.akamai.net has IPv6 address 2001:2030:0:27::c12d:f7aand some others with Incapsula bunker.ethzero.it is an alias for&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>So...</p>
<p>after some experience with akamai </p>
<pre><code><em>www.ethzero.it is an alias for aku-cs-akamaiflowers.com.edgesuite.net.</em></code><br><code><em>aku-cs-akamaiflowers.com.edgesuite.net is an alias for a169.dscksd.akamai.net.</em></code><br><code><em>a169.dscksd.akamai.net has address 193.45.15.98</em></code><br><code><em>a169.dscksd.akamai.net has address 193.45.15.122</em></code><br><code><em>a169.dscksd.akamai.net has IPv6 address 2001:2030:0:27::c12d:f62</em></code><br><code><em>a169.dscksd.akamai.net has IPv6 address 2001:2030:0:27::c12d:f7a</em></code></pre>
<p>and some others with Incapsula</p>
<pre><code><em>bunker.ethzero.it is an alias for ve7cu.x.incapdns.net.</em></code><br><code><em>ve7cu.x.incapdns.net has address 107.154.167.38</em></code></pre>
<p>It's now the moment to talk about Cloudflare</p>
<p> <a href="https://www.cloudflare.com/">Claudflare</a> provide a good free account that allow you to hide your source ip or you want to discover how it's work the free waf protection or have a https certificate having your origin in http and so on ... but what is really cool and quickwin than the competitors is the API support and the native integration with <a href="https://www.terraform.io/docs/providers/cloudflare/index.html">Terraform</a></p>
<p><img src="https://www.k8s.it/media/posts/5/git-terraform-cloudflare.png" alt="" width="2394" height="1254"></p>
<p>Terraform is the most integrated tool for cloud (not only) automation and infrastructure as a code </p>
<p>Here some steps that can show how it's esay deploy a configuration</p>
<p>First of all , retrive the api key from Cloudflare portal</p>
<p><img src="https://www.k8s.it/media/posts/5/cloudflare-api-key.png" alt="" width="1922" height="556"></p>
<p><img src="https://www.k8s.it/media/posts/5/cloudflare-global-apikey.png" alt="" width="1910" height="396"> </p>
<p>then start the main terraform file in a dedicated folder (better in a <a href="https://gitlab.com/">gitlab</a> repo)</p>
<pre><code>$ cat cloudflare-auth.tf</code><code><br>provider "cloudflare" {<br> email = "cloudflare_registration_email"<br> token = "cloudflare_token_api_key"<br>}<br></code></pre>
<p>make a $<code>terraform init</code> and now you can start to configure your account</p>
<p>manage your domain(s)</p>
<pre>$ <code>cat cloudflare_domains.tf</code><br><code>variable "domain" {</code><br><code> default = "k8s.it"</code><br><code>} </code></pre>
<p> </p>
<p>create dns configuration</p>
<pre>$ <code>cat cloudflare_dns.tf</code><br><code>resource "cloudflare_record" "www" {</code><br><code> domain = "${var.domain}"</code><br><code> name = "www"</code><br><code> value = "something.github.io"</code><br><code> type = "CNAME"</code><br><code> proxied = true</code><br><code>}</code><br><br><code>resource "cloudflare_record" "smtp" {</code><br><code> domain = "${var.domain}"</code><br><code> name = "smtp"</code><br><code> value = "IP.OF.PRIVATE.VPS"</code><br><code> type = "A"</code><br><code> proxied = false</code><br><code>}</code><br><br><code>resource "cloudflare_record" "services" {</code><br><code> domain = "${var.domain}"</code><br><code> name = "services"</code><br><code> value = "something.related.to.my.home"</code><br><code> type = "CNAME"</code><br><code> proxied = true</code><br><code>}</code><br><br><code>resource "cloudflare_record" "mx" {</code><br><code> domain = "${var.domain}"</code><br><code> name = "${var.domain}"</code><br><code> value = "smtp.k8s.it"</code><br><code> type = "MX"</code><br><code> priority = "1"</code><br><code>}</code><br><br><code>resource "cloudflare_record" "google-verification" {</code><br><code> domain = "${var.domain}"</code><br><code> name = "${var.domain}"</code><br><code> value = "google-site-verification=fdsfdssgfdg4teurtxh"</code><br><code> type = "TXT"</code><br><code>}</code><br><br><code>resource "cloudflare_record" "spf" {</code><br><code> domain = "${var.domain}"</code><br><code> name = "${var.domain}"</code><br><code> value = "v=spf1 ip4:IP.OF.PRIVATE.VPS mx ~all"</code><br><code> type = "TXT"</code><br><code>}</code></pre>
<p>Force the HTTPS with page rules</p>
<pre>$ <code>cat cloudflare_rules.tf</code><br><code>resource "cloudflare_page_rule" "always_use_https_www" {</code><br><code> zone = "${var.domain}"</code><br><code> target = "http://www.${var.domain}/*"</code><br><code> priority = 1</code><br><br><code> actions = {</code><br><code> always_use_https = "true",</code><br><code> }</code><br><code>}</code><br><br><code>resource "cloudflare_page_rule" "always_use_https_services" {</code><br><code> zone = "${var.domain}"</code><br><code> target = "http://services.${var.domain}/*"</code><br><code> priority = 2</code><br><br><code> actions = {</code><br><code> always_use_https = "true",</code><br><code> }</code><br><code>}</code></pre>
<p>And finally create the common zone settings </p>
<pre>$ <code>cat cloudflare_zone.tf</code><br><code>resource "cloudflare_zone_settings_override" "k8s-settings" {</code><br><code> name = "${var.domain}"</code><br><br><code> settings {</code><br><code> tls_1_3 = "on"</code><br><code> ssl = "flexible"</code><br><code> opportunistic_encryption = "on"</code><br><code> brotli = "on"</code><br><code> automatic_https_rewrites = "on"</code><br><code> security_level = "medium"</code><br><code> minify {</code><br><code> css = "on"</code><br><code> js = "on"</code><br><code> html = "on"</code><br><code> }</code><br><code> browser_cache_ttl = "14400"</code><br><code> }</code><br><code>}</code></pre>
<p>use the terraform plan to check if everything it's of and terrafom plan to apply the changes</p>
<pre>$ <code>terraform apply</code><br><code>cloudflare_record.mx: Refreshing state... (ID: c3624cc8fd163199ec082649a55f337a)</code><br><code>cloudflare_record.services: Refreshing state... (ID: 09041dfd16cbbf3b6915e79b14dc5466)</code><br><code>cloudflare_page_rule.always_use_https_services: Refreshing state... (ID: 92e196e6c7c8cba1f0759a60d713b0b9)</code><br><code>cloudflare_record.spf: Refreshing state... (ID: 3743b9f0be0a2c7a69245e169cf06ea5)</code><br><code>cloudflare_zone_settings_override.k8s-settings: Refreshing state... (ID: d4acc5e5713a0dede4f238b829f98947)</code><br><code>cloudflare_record.smtp: Refreshing state... (ID: 2d4628fc07c491d97e34b07da9ec60db)</code><br><code>cloudflare_page_rule.always_use_https_www: Refreshing state... (ID: 59c25cdd1096177a1019f834178de62f)</code><br><code>cloudflare_record.google-verification: Refreshing state... (ID: 131f35a9f8d2304a786a780db38b37e0)</code><br><code>cloudflare_record.www: Refreshing state... (ID: 76e3906f5ea172b627bed4922ebc1da7)</code><br><br><code>Apply complete! Resources: 0 added, 0 changed, 0 destroyed.</code></pre>
<p> </p>
<pre>$ <code>host www.k8s.it</code><br><code>www.k8s.it has address 104.27.164.146</code><br><code>www.k8s.it has address 104.27.165.146</code><br><code>www.k8s.it has IPv6 address 2400:cb00:2048:1::681b:a492</code><br><code>www.k8s.it has IPv6 address 2400:cb00:2048:1::681b:a592</code></pre>
<pre>$ <code>echo | openssl s_client -servername www.k8s.it -connect www.k8s.it:443</code><br><code>CONNECTED(00000003)</code><br><code>depth=2 C = GB, ST = Greater Manchester, L = Salford, O = COMODO CA Limited, CN = COMODO ECC Certification Authority</code><br><code>---</code><br><code>Certificate chain</code><br><code> 0 s:/OU=Domain Control Validated/OU=PositiveSSL Multi-Domain/CN=sni154797.cloudflaressl.com</code><br><code> i:/C=GB/ST=Greater Manchester/L=Salford/O=COMODO CA Limited/CN=COMODO ECC Domain Validation Secure Server CA 2</code><br><code> 1 s:/C=GB/ST=Greater Manchester/L=Salford/O=COMODO CA Limited/CN=COMODO ECC Domain Validation Secure Server CA 2</code><br><code> i:/C=GB/ST=Greater Manchester/L=Salford/O=COMODO CA Limited/CN=COMODO ECC Certification Authority</code><br><code> 2 s:/C=GB/ST=Greater Manchester/L=Salford/O=COMODO CA Limited/CN=COMODO ECC Certification Authority</code><br><code> i:/C=SE/O=AddTrust AB/OU=AddTrust External TTP Network/CN=AddTrust External CA Root</code></pre>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Kubernetes for mere mortals</title>
        <author>
            <name>lgirardi</name>
        </author>
        <link href="https://www.k8s.it/kubernetes-for-mere-mortals.html"/>
        <id>https://www.k8s.it/kubernetes-for-mere-mortals.html</id>
            <category term="traefik"/>
            <category term="orangepi"/>
            <category term="lab"/>
            <category term="kubernetes"/>
            <category term="k8s"/>
            <category term="armbian"/>
            <category term="arm"/>

        <updated>2018-04-10T23:52:47+02:00</updated>
            <summary>
                <![CDATA[
                    Well, what do you need to build your homemade cluster? armbian 4.10.1 kubeadm and kubelet 1.6.4 traefik as a ingress controller heapster, collectd, influxdb and grafana as a monitoring stack NAMESPACE NAME READY STATUS RESTARTS AGEhome-prd k8s-helloworld-3468567185-2fwhb 1/1 Running 0 10dhome-prd k8s-helloworld-3468567185-lbgpk 1/1 Running 2&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>Well, what do you need to build your homemade cluster?</p>
<p><img src="https://www.k8s.it/media/posts/3/k8s.arm-lg.jpeg" alt="" width="744" height="400"></p>
<p><img src="https://www.k8s.it/media/posts/3/IMG_20170605_150237.jpg" alt="" width="832" height="624"></p>
<ul>
<li>1x usb hub Anker 60W PowerPort 6</li>
<li>4x orangepi plus 2e</li>
</ul>
<p> armbian 4.10.1 </p>
<p>kubeadm and kubelet 1.6.4</p>
<p>traefik as a ingress controller</p>
<p>heapster, collectd, influxdb and grafana as a monitoring stack</p>
<div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img src="https://media.licdn.com/dms/image/C5612AQFJsWW1J0HYig/article-inline_image-shrink_1500_2232/0?e=2122596000&amp;v=beta&amp;t=O4agFAs9-5g0xGt6frAU-koLZROOeVmv2p7yhmZyEAY"  ></div>
<p> </p>
<div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img src="https://media.licdn.com/dms/image/C4E12AQFGmg6f2D2CcQ/article-inline_image-shrink_1000_1488/0?e=2122596000&amp;v=beta&amp;t=H2Sp0_RJzeKQLmbD7JiF6jW-bqdvqkgRzPTmXqhogpI"  ></div>
<pre spellcheck="false">NAMESPACE     NAME                                        READY     STATUS    RESTARTS   AGE
home-prd      k8s-helloworld<span class="hljs-number">-3468567185-2f</span>whb             <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running   <span class="hljs-number">0</span>          <span class="hljs-number">10d</span>
home-prd      k8s-helloworld<span class="hljs-number">-3468567185</span>-lbgpk             <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running   <span class="hljs-number">2</span>          <span class="hljs-number">73d</span>
home-prd      k8s-helloworld<span class="hljs-number">-3468567185</span>-vz9n5             <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running   <span class="hljs-number">2</span>          <span class="hljs-number">73d</span>
kube-system   etcd-k8s-node001                            <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running   <span class="hljs-number">19</span>         <span class="hljs-number">81d</span>
kube-system   heapster<span class="hljs-number">-3703175019-7f</span>z27                   <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running   <span class="hljs-number">2</span>          <span class="hljs-number">76d</span>
kube-system   kube-apiserver-k8s-node001                  <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running   <span class="hljs-number">8</span>          <span class="hljs-number">81d</span>
kube-system   kube-controller-manager-k8s-node001         <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running   <span class="hljs-number">22</span>         <span class="hljs-number">81d</span>
kube-system   kube-dns<span class="hljs-number">-279829092</span>-r8595                    <span class="hljs-number">3</span>/<span class="hljs-number">3</span>       Running   <span class="hljs-number">39</span>         <span class="hljs-number">81d</span>
kube-system   kube-flannel-ds<span class="hljs-number">-08</span>z1k                       <span class="hljs-number">2</span>/<span class="hljs-number">2</span>       Running   <span class="hljs-number">44</span>         <span class="hljs-number">81d</span>
kube-system   kube-flannel-ds<span class="hljs-number">-3</span>bqmf                       <span class="hljs-number">2</span>/<span class="hljs-number">2</span>       Running   <span class="hljs-number">0</span>          <span class="hljs-number">10d</span>
kube-system   kube-flannel-ds-f51rq                       <span class="hljs-number">2</span>/<span class="hljs-number">2</span>       Running   <span class="hljs-number">10</span>         <span class="hljs-number">81d</span>
kube-system   kube-flannel-ds-l5wjs                       <span class="hljs-number">2</span>/<span class="hljs-number">2</span>       Running   <span class="hljs-number">8</span>          <span class="hljs-number">81d</span>
kube-system   kube-proxy<span class="hljs-number">-3</span>gn3x                            <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running   <span class="hljs-number">12</span>         <span class="hljs-number">81d</span>
kube-system   kube-proxy<span class="hljs-number">-88762</span>                            <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running   <span class="hljs-number">2</span>          <span class="hljs-number">81d</span>
kube-system   kube-proxy-dghmh                            <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running   <span class="hljs-number">0</span>          <span class="hljs-number">10d</span>
kube-system   kube-proxy-dtf9c                            <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running   <span class="hljs-number">3</span>          <span class="hljs-number">81d</span>
kube-system   kube-scheduler-k8s-node001                  <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running   <span class="hljs-number">29</span>         <span class="hljs-number">81d</span>
kube-system   kubernetes-dashboard<span class="hljs-number">-1707270776</span>-jgxbp       <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running   <span class="hljs-number">2</span>          <span class="hljs-number">81d</span>
kube-system   traefik-ingress-controller<span class="hljs-number">-49053153</span>-kgb3p   <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running   <span class="hljs-number">1</span>          <span class="hljs-number">30d</span>
</pre>
<p> </p>
<p>backend docker example (nginx on arm) k8s-helloworld-arm</p>
<p><a href="https://services.k8s.it/hello/" target="_blank" rel="nofollow noopener noreferrer">live demo</a></p>
<p><a href="https://services.k8s.it/grafana/dashboard/db/all-k8s-nodes?refresh=1m&amp;orgId=2&amp;from=now-24h&amp;to=now" target="_blank" rel="nofollow noopener noreferrer">monitoring</a></p>
            ]]>
        </content>
    </entry>
</feed>
